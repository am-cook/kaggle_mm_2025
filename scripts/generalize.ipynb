{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data = '../data/'\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for women's analyses:\n",
    "\n",
    "m_or_w = 'WOMEN'\n",
    "\n",
    "raw_reg_szn = pd.read_csv(os.path.join(path_to_data, 'WRegularSeasonCompactResults.csv'))\n",
    "raw_conf_tourn = pd.read_csv(os.path.join(path_to_data, 'WConferenceTourneyGames.csv'))\n",
    "team_data = pd.read_csv(os.path.join(path_to_data, 'WTeams.csv'))\n",
    "raw_mm_tourn = pd.read_csv(os.path.join(path_to_data, 'WNCAATourneyCompactResults.csv'))\n",
    "raw_secondary_tourn = pd.read_csv(os.path.join(path_to_data, 'WSecondaryTourneyCompactResults.csv'))\n",
    "conf_data = pd.read_csv(os.path.join(path_to_data, 'WTeamConferences.csv'))\n",
    "tourney_slots = pd.read_csv(os.path.join(path_to_data, 'WNCAATourneySlots.csv'))\n",
    "\n",
    "# does not exist for women, but should be the same as for men\n",
    "seed_slots = pd.read_csv(os.path.join(path_to_data, 'MNCAATourneySeedRoundSlots.csv'))\n",
    "\n",
    "tourney_seeds = pd.read_csv(os.path.join(path_to_data, 'WNCAATourneySeeds.csv'))\n",
    "\n",
    "train_reg_szn_diffs_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'WOMENS_train_diffs.csv'), index_col= 0)\n",
    "val_reg_szn_diffs_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'WOMENS_val_diffs.csv'), index_col= 0)\n",
    "reg_szn_diffs_2025_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'WOMENS_test_diffs.csv'), index_col= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment for men's analyses\n",
    "\n",
    "# m_or_w = 'MEN'\n",
    "\n",
    "# raw_reg_szn = pd.read_csv(os.path.join(path_to_data, 'MRegularSeasonCompactResults.csv'))\n",
    "# raw_conf_tourn = pd.read_csv(os.path.join(path_to_data, 'MConferenceTourneyGames.csv'))\n",
    "# team_data = pd.read_csv(os.path.join(path_to_data, 'MTeams.csv'))\n",
    "# raw_mm_tourn = pd.read_csv(os.path.join(path_to_data, 'MNCAATourneyCompactResults.csv'))\n",
    "# raw_secondary_tourn = pd.read_csv(os.path.join(path_to_data, 'MSecondaryTourneyCompactResults.csv'))\n",
    "# conf_data = pd.read_csv(os.path.join(path_to_data, 'MTeamConferences.csv'))\n",
    "# tourney_slots = pd.read_csv(os.path.join(path_to_data, 'MNCAATourneySlots.csv'))\n",
    "# seed_slots = pd.read_csv(os.path.join(path_to_data, 'MNCAATourneySeedRoundSlots.csv'))\n",
    "# tourney_seeds = pd.read_csv(os.path.join(path_to_data, 'MNCAATourneySeeds.csv'))\n",
    "\n",
    "# train_reg_szn_diffs_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'MENS_train_diffs.csv'), index_col= 0)\n",
    "# val_reg_szn_diffs_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'MENS_val_diffs.csv'), index_col= 0)\n",
    "# reg_szn_diffs_2025_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'MENS_test_diffs.csv'), index_col= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = [2003, 2004, 2006, 2007, 2008, 2010, 2011, 2013, 2014, 2015, 2017, 2018, 2021, 2022, 2024]\n",
    "val_years = [2005, 2009, 2012, 2016, 2019, 2023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definitions\n",
    "\n",
    "def is_conference_game(team1, team2, season, conf_df = conf_data):\n",
    "    team1_conf = conf_df['ConfAbbrev'].loc[(conf_df['Season'] == season) & (conf_df['TeamID'] == team1)].values[0]\n",
    "    team2_conf = conf_df['ConfAbbrev'].loc[(conf_df['Season'] == season) & (conf_df['TeamID'] == team2)].values[0]\n",
    "\n",
    "    if team1_conf == team2_conf:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_elo_win_prob(elo1, elo2, home_court_advantage_boost_1 = 0, home_court_advantage_boost_2 = 0):\n",
    "    '''\n",
    "    returns probability of team1 (with elo1) beating team 2 (with elo2)\n",
    "    '''\n",
    "\n",
    "    elo1 += home_court_advantage_boost_1\n",
    "    elo2 += home_court_advantage_boost_2\n",
    "\n",
    "    return  1 / (1 + 10 ** ((elo2 - elo1) / 400))\n",
    "\n",
    "\n",
    "\n",
    "def update_elo(prev_elo1, prev_elo2, k, winner, hc_boost_1, hc_boost_2):\n",
    "    '''\n",
    "    returns updated elos for team 1 (with prev_elo1) and team 2 (prev_elo2) based on k\n",
    "    we don't care about margin of victory, just wins and losses:\n",
    "    '''\n",
    "\n",
    "    team1_winprob = get_elo_win_prob(prev_elo1, prev_elo2, hc_boost_1, hc_boost_2)\n",
    "    team2_winprob = 1 - team1_winprob\n",
    "\n",
    "    if winner == 1:\n",
    "\n",
    "        new_elo1 = prev_elo1 + k*(1 - team1_winprob)\n",
    "        new_elo2 = prev_elo2 + k*(0 - team1_winprob)\n",
    "\n",
    "    elif winner == 2:\n",
    "\n",
    "        new_elo1 = prev_elo1 + k*(0 - team1_winprob)\n",
    "        new_elo2 = prev_elo2 + k*(1 - team1_winprob)\n",
    "\n",
    "    return new_elo1, new_elo2 \n",
    "\n",
    "\n",
    "\n",
    "def make_final_elo_df(year, score_result_df, k_scheme, home_team_adjustment = 0, fix_k = False, team_elo_dict = None):\n",
    "    '''\n",
    "    Make a df of teams' elos at the end of the regular season\n",
    "    \n",
    "    '''\n",
    "\n",
    "    season_df = score_result_df.loc[score_result_df['Season'] == year]\n",
    "\n",
    "    # get all unique team ids in the season:\n",
    "    all_team_ids = np.unique(season_df[['WTeamID', 'LTeamID']].values)\n",
    "\n",
    "    if team_elo_dict is None:\n",
    "        # initialize all teams' elo to 1500 to begin the season\n",
    "        team_elo_dict = {int(team):[1500] for team in all_team_ids}\n",
    "\n",
    "    for rownum, rowvals in tqdm(season_df.iterrows()):\n",
    "        \n",
    "        winning_team = int(rowvals['WTeamID'])\n",
    "        losing_team = int(rowvals['LTeamID'])\n",
    "\n",
    "        winning_team_prev_elo = team_elo_dict[winning_team][-1]\n",
    "        losing_team_prev_elo = team_elo_dict[losing_team][-1]\n",
    "\n",
    "        conf_game = is_conference_game(team1 = winning_team, \n",
    "                                       team2 = losing_team,\n",
    "                                       season = year, \n",
    "                                       conf_df = conf_data)\n",
    "        if fix_k:\n",
    "            this_k = k_scheme['fixed']\n",
    "        else:\n",
    "            if conf_game:\n",
    "                this_k = k_scheme['conf']\n",
    "            else:\n",
    "                this_k = k_scheme['ooc']\n",
    "\n",
    "        if home_team_adjustment != 0:\n",
    "            # if the winning team was at home ... \n",
    "            if rowvals['WLoc'] == 'H':\n",
    "                winning_elo_adj = home_team_adjustment\n",
    "                losing_elo_adj = 0\n",
    "            \n",
    "            # if the losing team was at home ... \n",
    "            elif rowvals['WLoc'] == 'A':\n",
    "                winning_elo_adj = 0\n",
    "                losing_elo_adj = home_team_adjustment\n",
    "            \n",
    "            # if neutral site, neither team gets an elo boost\n",
    "            elif rowvals['WLoc'] == 'N':\n",
    "                winning_elo_adj = 0\n",
    "                losing_elo_adj = 0\n",
    "\n",
    "        # useful for conference tourney games that lack this data ... \n",
    "        else:\n",
    "            winning_elo_adj = 0\n",
    "            losing_elo_adj = 0\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "        new_winner_elo, new_loser_elo = update_elo(prev_elo1=winning_team_prev_elo,\n",
    "                                                prev_elo2 = losing_team_prev_elo,\n",
    "                                                k = this_k,\n",
    "                                                winner = 1,\n",
    "                                                hc_boost_1 = winning_elo_adj,\n",
    "                                                hc_boost_2= losing_elo_adj\n",
    "                                                )\n",
    "        \n",
    "        team_elo_dict[winning_team].append(new_winner_elo)\n",
    "        team_elo_dict[losing_team].append(new_loser_elo)\n",
    "\n",
    "    final_elos = {team_id:elo_scores[-1] for team_id, elo_scores in team_elo_dict.items()}\n",
    "    final_elo_df = pd.DataFrame.from_dict(final_elos, orient = 'index')\n",
    "    final_elo_df.columns = ['elo']\n",
    "    final_elo_df = pd.merge(left = final_elo_df, right = team_data, left_index=True, right_on = 'TeamID')\n",
    "    final_elo_df = final_elo_df.sort_values('elo', ascending = False)\n",
    "\n",
    "    return team_elo_dict, final_elo_df\n",
    "\n",
    "\n",
    "def create_x_y_data(season, reg_szn_data, final_elo_df_szn, stat_feats, include_winner_in_x = False, \n",
    "                    include_secondary_tourney_data = True,\n",
    "                    include_conf_tourney_data = False,\n",
    "                    m_or_w = m_or_w):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if include_conf_tourney_data, we will NOT adjust ELO from conference tournament results; we will INCLUDE it in the training process\n",
    "    '''\n",
    "    \n",
    "    # train optimal mod:\n",
    "    if m_or_w == 'MEN':\n",
    "        prefix = 'M'\n",
    "    elif m_or_w == 'WOMEN':\n",
    "        prefix = 'W'\n",
    "\n",
    "\n",
    "    raw_mm_tourn = pd.read_csv(os.path.join(path_to_data, f'{prefix}NCAATourneyCompactResults.csv'))\n",
    "    raw_secondary_tourn = pd.read_csv(os.path.join(path_to_data, f'{prefix}SecondaryTourneyCompactResults.csv'))\n",
    "    raw_conf_tourn = pd.read_csv(os.path.join(path_to_data, f'{prefix}ConferenceTourneyGames.csv'))\n",
    "\n",
    "    my_regszn_szn = reg_szn_data.loc[reg_szn_data['Season'] == season]\n",
    "    conf_data_szn = conf_data.loc[conf_data['Season'] == season]\n",
    "    regszn_avgs_szn = my_regszn_szn.groupby('TeamID')[stat_feats].mean()\n",
    "\n",
    "    if include_secondary_tourney_data:\n",
    "        \n",
    "\n",
    "        raw_mm_tourn_szn = raw_mm_tourn.loc[raw_mm_tourn['Season'] == season]\n",
    "        raw_secondary_tourn_szn = raw_secondary_tourn.loc[raw_secondary_tourn['Season'] == season]\n",
    "        postseason_games = pd.concat([raw_mm_tourn_szn, raw_secondary_tourn_szn], axis = 0, ignore_index=True)\n",
    "    else:\n",
    "        postseason_games = raw_mm_tourn.loc[raw_mm_tourn['Season'] == season]\n",
    "\n",
    "    if include_conf_tourney_data:\n",
    "        raw_conf_szn = raw_conf_tourn.loc[raw_conf_tourn['Season'] == season]\n",
    "        postseason_games = pd.concat([postseason_games, raw_conf_szn], axis = 0, ignore_index=True)\n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    regszn_avgs_szn = pd.merge(left = regszn_avgs_szn, right = conf_data_szn[['TeamID', 'ConfAbbrev']], on = 'TeamID')\n",
    "\n",
    "    team_feats_szn = pd.merge(left = regszn_avgs_szn, right = final_elo_df_szn[['elo', 'TeamID']], on = 'TeamID')\n",
    "    \n",
    "\n",
    "    all_pairwise_data = []\n",
    "    all_pairwise_diffs_data = []\n",
    "    winner_list = []\n",
    "\n",
    "\n",
    "    for rownum, rowvals in postseason_games.iterrows():\n",
    "\n",
    "\n",
    "\n",
    "        # team1 will always be the team with the lower team ID\n",
    "\n",
    "        team_ids = [rowvals['WTeamID'], rowvals['LTeamID']]\n",
    "        team1 = min(team_ids)\n",
    "        team2 = max(team_ids)\n",
    "\n",
    "        if team1 == rowvals['WTeamID']:\n",
    "            winner = 0\n",
    "        elif team2 == rowvals['WTeamID']:\n",
    "            winner = 1\n",
    "        \n",
    "        team1_vals = team_feats_szn.loc[team_feats_szn['TeamID'] == team1].values.tolist()[0]\n",
    "        team1_dict = dict(zip([f'{col}_1' for col in team_feats_szn.columns], team1_vals))\n",
    "\n",
    "        team2_vals = team_feats_szn.loc[team_feats_szn['TeamID'] == team2].values.tolist()[0]\n",
    "        team2_dict = dict(zip([f'{col}_2' for col in team_feats_szn.columns], team2_vals))\n",
    "\n",
    "        diffs_dict = {}\n",
    "        for team_feat in team1_dict.keys():\n",
    "            \n",
    "            # remove the _# so that we can use for team2 as well\n",
    "            team_feat_stem = team_feat[:-2]\n",
    "\n",
    "            # if this is a stat-related column, find the difference between team 1 and team 2\n",
    "            if team_feat_stem in stat_feats:\n",
    "                diffs_dict[f'{team_feat_stem}_diff'] = team1_dict[f'{team_feat_stem}_1'] - team2_dict[f'{team_feat_stem}_2']\n",
    "\n",
    "            # if it's not a numeric stat column, add existing elements to dict\n",
    "            else:\n",
    "                diffs_dict[f'{team_feat_stem}_1'] = team1_dict[f'{team_feat_stem}_1']\n",
    "                diffs_dict[f'{team_feat_stem}_2'] = team2_dict[f'{team_feat_stem}_2']\n",
    "        \n",
    "        if include_winner_in_x:\n",
    "            diffs_dict.update({'winner': winner})\n",
    "            team1_dict.update({'winner':winner})\n",
    "\n",
    "        all_pairwise_diffs_data.append(diffs_dict)\n",
    "\n",
    "        team1_dict.update(team2_dict)\n",
    "        all_pairwise_data.append(team1_dict)\n",
    "\n",
    "        \n",
    "        winner_list.append(winner)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    pairwise_df_szn = pd.DataFrame(all_pairwise_data)\n",
    "    pairwise_diffs_df_szn = pd.DataFrame(all_pairwise_diffs_data)\n",
    "\n",
    "    # concat confs, then drop team ids and conference data\n",
    "    concat_confs = pairwise_df_szn['ConfAbbrev_1']+ '_' + pairwise_df_szn['ConfAbbrev_2']\n",
    "    pairwise_df_szn.insert(column = 'merged_conf', value = concat_confs, loc = pairwise_df_szn.shape[1])\n",
    "\n",
    "    pairwise_df_szn.drop(columns = ['ConfAbbrev_1', 'ConfAbbrev_2', 'TeamID_1', 'TeamID_2'], inplace = True)\n",
    "\n",
    "    # concat confs, then drop team ids and conference data\n",
    "    concat_confs = pairwise_diffs_df_szn['ConfAbbrev_1']+ '_' + pairwise_diffs_df_szn['ConfAbbrev_2']\n",
    "    pairwise_diffs_df_szn.insert(column = 'merged_conf', value = concat_confs, loc = pairwise_diffs_df_szn.shape[1])\n",
    "\n",
    "    pairwise_diffs_df_szn.drop(columns = ['ConfAbbrev_1', 'ConfAbbrev_2', 'TeamID_1', 'TeamID_2'], inplace = True)\n",
    "\n",
    "    return pairwise_df_szn, pairwise_diffs_df_szn, winner_list\n",
    "\n",
    "\n",
    "def model_train_workflow(train_x, train_y, val_x, val_y, params_to_categorize = [], scaler = None, grid_search=True, \n",
    "                         model_type = 'xgb', one_hot_encode_cat = False):\n",
    "    \n",
    "    \n",
    "    if len(params_to_categorize) > 0:\n",
    "        for param in params_to_categorize:\n",
    "            train_x[param] = train_x[param].astype('category')\n",
    "            val_x[param] = val_x[param].astype('category')\n",
    "    scale_cols = [col for col in train_x.columns if col not in params_to_categorize]\n",
    "\n",
    "\n",
    "    if one_hot_encode_cat:\n",
    "        # Define transformers\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', scaler, scale_cols),  \n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), params_to_categorize) \n",
    "        ], remainder='passthrough') \n",
    "    else:\n",
    "        # Define transformers\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', scaler, scale_cols)  \n",
    "        ], remainder='passthrough')  \n",
    "\n",
    "\n",
    "    if model_type == 'xgb':\n",
    "        clf = xgb.XGBClassifier(tree_method=\"hist\", enable_categorical=True)\n",
    "\n",
    "       \n",
    "\n",
    "        if grid_search:\n",
    "\n",
    "            # Create a pipeline with preprocessing and model\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),  \n",
    "                ('xgb', clf)  \n",
    "            ])\n",
    "\n",
    "            # Define hyperparameter grid\n",
    "            param_grid = {\n",
    "                'xgb__n_estimators': [50, 100, 200],  \n",
    "                'xgb__max_depth': [3, 5, 7],  \n",
    "                'xgb__learning_rate': [0.01, 0.1, 0.2],  \n",
    "            }\n",
    "\n",
    "            # Set up GridSearchCV\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=param_grid,\n",
    "                scoring='accuracy',\n",
    "                cv=5,\n",
    "                verbose=1,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            # Fit the grid search\n",
    "            grid_search.fit(train_x, train_y)\n",
    "\n",
    "            # Best parameters and score\n",
    "            print(\"Best Parameters:\", grid_search.best_params_)\n",
    "            print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "            # Evaluate on test set\n",
    "            best_model = grid_search.best_estimator_\n",
    "        else:\n",
    "            # Define an XGBoost classifier with default hyperparameters\n",
    "            if one_hot_encode_cat:\n",
    "                clf = xgb.XGBClassifier(\n",
    "                    use_label_encoder=False, \n",
    "                    eval_metric='logloss', \n",
    "                    n_estimators=100, \n",
    "                    max_depth=5, \n",
    "                    learning_rate=0.1\n",
    "                )\n",
    "\n",
    "                \n",
    "            else:\n",
    "                clf = xgb.XGBClassifier(\n",
    "                    use_label_encoder=True, \n",
    "                    eval_metric='logloss', \n",
    "                    n_estimators=100, \n",
    "                    max_depth=5, \n",
    "                    learning_rate=0.1\n",
    "                )\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('xgb', clf)\n",
    "            ])\n",
    "\n",
    "\n",
    "            pipeline.fit(train_x, train_y)\n",
    "            best_model = clf\n",
    "    \n",
    "\n",
    "\n",
    "    elif model_type == 'logreg':\n",
    "        \n",
    "        clf = LogisticRegression()\n",
    "\n",
    "\n",
    "        if grid_search:\n",
    " \n",
    "            \n",
    "            # Create a pipeline with preprocessing and model\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),  \n",
    "                ('logreg', clf)  \n",
    "            ])\n",
    "\n",
    "            # Define hyperparameter grid\n",
    "            param_grid = {\n",
    "                'logreg__C': [0.01, 0.1, 1, 10, 100],  \n",
    "                'logreg__penalty': ['l1', 'l2'], \n",
    "                'logreg__solver': ['liblinear', 'saga'],\n",
    "                'logreg__max_iter': [100, 500, 1000]  \n",
    "            }\n",
    "\n",
    "            # Set up GridSearchCV\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_grid=param_grid,\n",
    "                scoring='accuracy',\n",
    "                cv=5,  \n",
    "                verbose=1,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            # Fit the grid search\n",
    "            grid_search.fit(train_x, train_y)\n",
    "\n",
    "            print(\"Best Parameters:\", grid_search.best_params_)\n",
    "            print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "            best_model = grid_search.best_estimator_\n",
    "        \n",
    "        else:\n",
    "            if one_hot_encode_cat:\n",
    "                clf = LogisticRegression(solver = 'saga',\n",
    "                                         penalty = 'l2')\n",
    "\n",
    "                \n",
    "            else:\n",
    "                print('need one hot encoding for logistic regression')\n",
    "                return\n",
    "\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('xgb', clf)\n",
    "            ])\n",
    "\n",
    "\n",
    "            pipeline.fit(train_x, train_y)\n",
    "            best_model = clf\n",
    "\n",
    "\n",
    "    pred_probs = best_model.predict_proba(val_x)\n",
    "    preds = best_model.predict(val_x)\n",
    "    class_1_probas = [probs[1] for probs in pred_probs]\n",
    "    brier_loss = brier_score_loss(y_true = val_y, y_proba = class_1_probas)\n",
    "    accuracy = accuracy_score(y_true = val_y, y_pred = preds)\n",
    "\n",
    "    print(\"Brier loss:\", brier_loss)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    if grid_search:\n",
    "        return brier_loss, accuracy, grid_search.best_params_\n",
    "    else:\n",
    "        return brier_loss, accuracy, \n",
    "\n",
    "\n",
    "def full_workflow(k_dict, reg_szn_data_type, include_conf_res, include_secondary_res,\n",
    "                  feature_sets_to_include, run_name, mods_to_include = ['xgb', 'logreg'],\n",
    "                  scalers_to_include = [MinMaxScaler(), StandardScaler()],\n",
    "                  m_or_w = m_or_w):\n",
    "    \n",
    "    '''\n",
    "    if include_conf_res, we will NOT adjust elo in conference tournaments; we WILL use conf tourney results to train\n",
    "    '''\n",
    "    \n",
    "\n",
    "    \n",
    "    if m_or_w == 'MEN':\n",
    "        prefix = 'M'\n",
    "    elif m_or_w == 'WOMEN':\n",
    "        prefix = 'W'\n",
    "    raw_reg_szn = pd.read_csv(os.path.join(path_to_data, f'{prefix}RegularSeasonCompactResults.csv'))\n",
    "    raw_conf_tourn = pd.read_csv(os.path.join(path_to_data, f'{prefix}ConferenceTourneyGames.csv'))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # if reg_szn_data_type == 'BIGSZN':\n",
    "    #     reg_szn_train_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'big_reg_szn_train.csv'), index_col = 0)\n",
    "    #     reg_szn_val_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'big_reg_szn_val.csv'), index_col = 0)\n",
    "        \n",
    "    if reg_szn_data_type == 'DIFFS':\n",
    "        \n",
    "        if m_or_w == 'MEN':\n",
    "            reg_szn_train_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'MENS_train_diffs.csv'), index_col= 0)\n",
    "            reg_szn_val_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'MENS_val_diffs.csv'), index_col= 0)\n",
    "        elif m_or_w == 'WOMEN':\n",
    "            reg_szn_train_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'WOMENS_train_diffs.csv'), index_col= 0)\n",
    "            reg_szn_val_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'WOMENS_val_diffs.csv'), index_col= 0)\n",
    "    stat_feats = list(reg_szn_train_data.columns[2:])\n",
    "    \n",
    "    ######################################## training data\n",
    "    train_list_of_nondiffs_dfs = []\n",
    "    train_list_of_diffs_dfs = []\n",
    "    train_list_of_ys = []\n",
    "\n",
    "    for season in reg_szn_train_data['Season'].unique():\n",
    "\n",
    "        team_elo_dict, final_elo_df = make_final_elo_df(year = season, score_result_df=raw_reg_szn, team_elo_dict=None, k_scheme=k_dict, fix_k = False)\n",
    "\n",
    "        # we only want to perform conference tournament ELO adjustments if we ARE NOT using conference data as trainable results\n",
    "        if not include_conf_res:\n",
    "            postcon_team_elo_dict, postcon_final_elo_df = make_final_elo_df(year = season, score_result_df=raw_conf_tourn, team_elo_dict=team_elo_dict, k_scheme=k_dict, fix_k= True)\n",
    "            x_pairwise, x_pairwise_diffs, y = create_x_y_data(season = season, final_elo_df_szn = postcon_final_elo_df, reg_szn_data=reg_szn_train_data, \n",
    "                                                        stat_feats = stat_feats, include_winner_in_x = False, include_conf_tourney_data=False,\n",
    "                                                        include_secondary_tourney_data=include_secondary_res)\n",
    "        elif include_conf_res:  \n",
    "            x_pairwise, x_pairwise_diffs, y = create_x_y_data(season = season, final_elo_df_szn = final_elo_df, reg_szn_data=reg_szn_train_data, \n",
    "                                                            stat_feats = stat_feats, include_winner_in_x = False, include_conf_tourney_data=True,\n",
    "                                                            include_secondary_tourney_data=include_secondary_res)\n",
    "\n",
    "        train_list_of_nondiffs_dfs.append(x_pairwise)\n",
    "        train_list_of_diffs_dfs.append(x_pairwise_diffs)\n",
    "        train_list_of_ys.extend(y)\n",
    "\n",
    "    full_nondiffs_df_train = pd.concat(train_list_of_nondiffs_dfs, axis = 0, ignore_index=True)\n",
    "    full_diffs_df_train = pd.concat(train_list_of_diffs_dfs, axis = 0, ignore_index=True)\n",
    "    full_y_df_train = pd.DataFrame(train_list_of_ys)\n",
    "\n",
    "    ######################################### validation data\n",
    "    val_list_of_nondiffs_dfs = []\n",
    "    val_list_of_diffs_dfs = []\n",
    "    val_list_of_ys = []\n",
    "\n",
    "    for season in reg_szn_val_data['Season'].unique():\n",
    "\n",
    "        team_elo_dict, final_elo_df = make_final_elo_df(year = season, score_result_df=raw_reg_szn, team_elo_dict=None, k_scheme=k_dict, fix_k = False)\n",
    "\n",
    "        if not include_conf_res:\n",
    "            postcon_team_elo_dict, postcon_final_elo_df = make_final_elo_df(year = season, score_result_df=raw_conf_tourn, team_elo_dict=team_elo_dict, k_scheme=k_dict, fix_k= True)\n",
    "\n",
    "            x_pairwise, x_pairwise_diffs, y = create_x_y_data(season = season, final_elo_df_szn = postcon_final_elo_df, reg_szn_data=reg_szn_val_data, stat_feats=stat_feats,\n",
    "                                                            include_winner_in_x = False, include_conf_tourney_data=False,\n",
    "                                                            include_secondary_tourney_data=include_secondary_res)\n",
    "        elif include_conf_res:\n",
    "            x_pairwise, x_pairwise_diffs, y = create_x_y_data(season = season, final_elo_df_szn = final_elo_df, reg_szn_data=reg_szn_val_data, stat_feats=stat_feats,\n",
    "                                                            include_winner_in_x = False, include_conf_tourney_data=True,\n",
    "                                                            include_secondary_tourney_data=include_secondary_res)\n",
    "\n",
    "        val_list_of_nondiffs_dfs.append(x_pairwise)\n",
    "        val_list_of_diffs_dfs.append(x_pairwise_diffs)\n",
    "        val_list_of_ys.extend(y)\n",
    "\n",
    "    full_nondiffs_df_val = pd.concat(val_list_of_nondiffs_dfs, axis = 0, ignore_index=True)\n",
    "    full_diffs_df_val = pd.concat(val_list_of_diffs_dfs, axis = 0, ignore_index=True)\n",
    "    full_y_df_val = pd.DataFrame(val_list_of_ys)\n",
    "\n",
    "    ######################################### model testing\n",
    "\n",
    "    dataset_dict_list = []\n",
    "\n",
    "    if 'barebones_merged_diffs' in feature_sets_to_include:\n",
    "\n",
    "        # matchup diffs\n",
    "        barebones_diffs_merged_train = full_diffs_df_train[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "        barebones_diffs_merged_val = full_diffs_df_val[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "        barebones_diffs_merged_data_dict = {'name':'barebones_merged_diffs',\n",
    "                        'x_train':barebones_diffs_merged_train,\n",
    "                        'x_val': barebones_diffs_merged_val,\n",
    "                        'y_train':full_y_df_train,\n",
    "                        'y_val':full_y_df_val}\n",
    "        dataset_dict_list.append(barebones_diffs_merged_data_dict)\n",
    "\n",
    "    if 'barebones_merged_nondiffs' in feature_sets_to_include:\n",
    "\n",
    "        barebones_nondiffs_merged_train = full_nondiffs_df_train[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "        barebones_nondiffs_merged_val = full_nondiffs_df_val[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "        barebones_nondiffs_merged_data_dict = {'name':'barebones_merged_nondiffs',\n",
    "                        'x_train':barebones_nondiffs_merged_train,\n",
    "                        'x_val': barebones_nondiffs_merged_val,\n",
    "                        'y_train':full_y_df_train,\n",
    "                        'y_val':full_y_df_val}\n",
    "        dataset_dict_list.append(barebones_nondiffs_merged_data_dict)\n",
    "\n",
    "\n",
    "    if 'barebones_sepconf_diffs' in feature_sets_to_include:\n",
    "        barebones_diffs_train_sepconf = full_diffs_df_train[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "        barebones_diffs_train_sepconf[['conf_1', 'conf_2']] = barebones_diffs_train_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "        barebones_diffs_train_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "        barebones_diffs_val_sepconf = full_diffs_df_val[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "        barebones_diffs_val_sepconf[['conf_1', 'conf_2']] = barebones_diffs_val_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "        barebones_diffs_val_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "        barebones_diffs_sepconf_data_dict = {'name': 'barebones_sepconf_diffs',\n",
    "                        'x_train':barebones_diffs_train_sepconf,\n",
    "                        'x_val': barebones_diffs_val_sepconf,\n",
    "                        'y_train':full_y_df_train,\n",
    "                        'y_val':full_y_df_val}\n",
    "        dataset_dict_list.append(barebones_diffs_sepconf_data_dict)\n",
    "\n",
    "    if 'barebones_sepconf_nondiffs' in feature_sets_to_include:\n",
    "\n",
    "        barebones_nondiffs_train_sepconf = full_nondiffs_df_train[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "        barebones_nondiffs_train_sepconf[['conf_1', 'conf_2']] = barebones_nondiffs_train_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "        barebones_nondiffs_train_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "        barebones_nondiffs_val_sepconf = full_nondiffs_df_val[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "        barebones_nondiffs_val_sepconf[['conf_1', 'conf_2']] = barebones_nondiffs_val_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "        barebones_nondiffs_val_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "        barebones_nondiffs_sepconf_data_dict = {'name': 'barebones_sepconf_nondiffs',\n",
    "                        'x_train':barebones_nondiffs_train_sepconf,\n",
    "                        'x_val': barebones_nondiffs_val_sepconf,\n",
    "                        'y_train':full_y_df_train,\n",
    "                        'y_val':full_y_df_val}\n",
    "        dataset_dict_list.append(barebones_nondiffs_sepconf_data_dict)\n",
    "    \n",
    "    \n",
    "    if 'full_diffs' in feature_sets_to_include:\n",
    "        full_diffs_data_dict = {\n",
    "            'name': f'{reg_szn_data_type}_full_diffs',\n",
    "            'x_train':full_diffs_df_train,\n",
    "            'x_val': full_diffs_df_val,\n",
    "            'y_train':full_y_df_train,\n",
    "            'y_val':full_y_df_val\n",
    "        }\n",
    "        dataset_dict_list.append(full_diffs_data_dict)\n",
    "\n",
    "    if 'full_sepconf_diffs' in feature_sets_to_include:\n",
    "        full_diffs_train_sepconf = full_diffs_df_train.copy(deep = True)\n",
    "        full_diffs_train_sepconf[['conf_1', 'conf_2']] = full_diffs_train_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "        full_diffs_train_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "        full_diffs_val_sepconf = full_diffs_df_val.copy(deep = True)\n",
    "        full_diffs_val_sepconf[['conf_1', 'conf_2']] = full_diffs_val_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "        full_diffs_val_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "        full_sepconf_data_dict = {'name': 'full_sepconf_diffs',\n",
    "                        'x_train':full_diffs_train_sepconf,\n",
    "                        'x_val': full_diffs_val_sepconf,\n",
    "                        'y_train':full_y_df_train,\n",
    "                        'y_val':full_y_df_val}\n",
    "        dataset_dict_list.append(full_sepconf_data_dict)\n",
    "\n",
    "    if 'full_nondiffs' in feature_sets_to_include:\n",
    "        full_nondiffs_data_dict = {\n",
    "            'name': f'{reg_szn_data_type}_full_nondiffs',\n",
    "            'x_train':full_nondiffs_df_train,\n",
    "            'x_val': full_nondiffs_df_val,\n",
    "            'y_train':full_y_df_train,\n",
    "            'y_val':full_y_df_val\n",
    "        }\n",
    "        dataset_dict_list.append(full_nondiffs_data_dict)\n",
    "\n",
    "\n",
    "    # want to ensure scaling steps are not overwriting the values in the dataframe ...\n",
    "    # will save each df as csv then read it in within each iteration\n",
    "    # will slow it down tremendously but that's ok for now\n",
    "\n",
    "    growing_model_results_list = []\n",
    "\n",
    "    onehot_allowed_dict = {'xgb':[True],\n",
    "                            'logreg':[True]}\n",
    "    \n",
    "    # now iterate through each data dictionary and build multiple models for each set of data dict data\n",
    "    for data_dict in dataset_dict_list:\n",
    "        \n",
    "        this_datadict_path = os.path.join(path_to_data, 'custom_datasets', 'grid_search_res', run_name, k_dict['name'], reg_szn_data_type, data_dict['name'])\n",
    "        if not os.path.exists(this_datadict_path):\n",
    "            os.makedirs(this_datadict_path)\n",
    "        \n",
    "    #     \n",
    "        data_dict['x_train'].to_csv(os.path.join(this_datadict_path, 'x_train.csv'), index = False)\n",
    "        data_dict['x_val'].to_csv(os.path.join(this_datadict_path, 'x_val.csv'), index = False)\n",
    "        data_dict['y_train'].to_csv(os.path.join(this_datadict_path, 'y_train.csv'), index = False)\n",
    "        data_dict['y_val'].to_csv(os.path.join(this_datadict_path, 'y_val.csv'), index = False)\n",
    "    \n",
    "        for this_scaler in scalers_to_include:\n",
    "            for this_model in mods_to_include:\n",
    "                for this_onehot in onehot_allowed_dict[this_model]:\n",
    "                    \n",
    "                    x_train = pd.read_csv(os.path.join(this_datadict_path, 'x_train.csv'))\n",
    "                    x_val = pd.read_csv(os.path.join(this_datadict_path, 'x_val.csv'))\n",
    "                    y_train = pd.read_csv(os.path.join(this_datadict_path, 'y_train.csv'))\n",
    "                    y_val = pd.read_csv(os.path.join(this_datadict_path, 'y_val.csv'))\n",
    "                    \n",
    "                    raveled_y_train = y_train.values.ravel()\n",
    "                    raveled_y_val = y_val.values.ravel()\n",
    "\n",
    "                    # this will dictate which cols are categorized\n",
    "                    if 'sepconf' in data_dict['name']:\n",
    "                        brier, acc, params = model_train_workflow(train_x = x_train, train_y = raveled_y_train, \n",
    "                                            val_x = x_val, val_y = raveled_y_val, \n",
    "                                            params_to_categorize = ['conf_1', 'conf_2'], \n",
    "                                            scaler = this_scaler, grid_search=True,\n",
    "                                            model_type=this_model, one_hot_encode_cat=this_onehot)\n",
    "                    else:\n",
    "                        brier, acc, params = model_train_workflow(train_x = x_train, train_y = raveled_y_train, \n",
    "                                            val_x = x_val, val_y = raveled_y_val, \n",
    "                                            params_to_categorize = ['merged_conf'], \n",
    "                                            scaler = this_scaler, grid_search=True,\n",
    "                                            model_type=this_model, one_hot_encode_cat=this_onehot)\n",
    "                    \n",
    "                    this_param_combo_dict = {'scaler': this_scaler,\n",
    "                                             'model': this_model,\n",
    "                                             'onehot': this_onehot}\n",
    "\n",
    "                    this_mod_results_list = [k_dict['name'], data_dict['name'],  str(data_dict), str(this_param_combo_dict), str(params), brier, acc]\n",
    "                    growing_model_results_list.append(this_mod_results_list)\n",
    "    \n",
    "    res_df = pd.DataFrame(growing_model_results_list)\n",
    "\n",
    "\n",
    "    res_df.to_csv(os.path.join(path_to_data, 'custom_datasets', 'grid_search_res', run_name, k_dict['name'], reg_szn_data_type, 'grid_search_res.csv'))\n",
    "\n",
    "                    \n",
    "        \n",
    "def make_2025_x():\n",
    "\n",
    "    # create the test X dataframe:\n",
    "    all_team_ids_2025 = [int(teamid) for teamid in reg_szn_diffs_2025_data['TeamID'].unique()]\n",
    "    pairwise_teams_2025 = list(itertools.combinations(all_team_ids_2025, 2))\n",
    "    stat_feats = list(reg_szn_diffs_2025_data.columns[2:])\n",
    "    conf_data_szn = conf_data.loc[conf_data['Season'] == 2025]\n",
    "    regszn_avgs_szn = reg_szn_diffs_2025_data.groupby('TeamID')[stat_feats].mean()\n",
    "\n",
    "\n",
    "    opt_k_dict = {'ooc':20,\n",
    "                'conf': 40}\n",
    "    team_elo_dict, final_elo_df = make_final_elo_df(year = 2025, score_result_df=raw_reg_szn, team_elo_dict=None, \n",
    "                                                    k_scheme=opt_k_dict, fix_k = False)\n",
    "\n",
    "    # merge and set default noconf values for the schools missing conf data\n",
    "    regszn_avgs_szn = reg_szn_diffs_2025_data.groupby('TeamID')[stat_feats].mean()\n",
    "    regszn_avgs_szn = pd.merge(left = regszn_avgs_szn, right = conf_data_szn[['TeamID', 'ConfAbbrev']], \n",
    "                            on = 'TeamID', how = 'left')\n",
    "    regszn_avgs_szn['ConfAbbrev'] = regszn_avgs_szn['ConfAbbrev'].fillna('noconf')\n",
    "    team_feats_szn = pd.merge(left = regszn_avgs_szn, right = final_elo_df[['elo', 'TeamID']], on = 'TeamID')\n",
    "\n",
    "    all_pairwise_data = []\n",
    "    all_pairwise_diffs_data = []\n",
    "\n",
    "    for team1, team2 in tqdm(pairwise_teams_2025):\n",
    "\n",
    "        team1_vals = team_feats_szn.loc[team_feats_szn['TeamID'] == team1].values.tolist()[0]\n",
    "        team1_dict = dict(zip([f'{col}_1' for col in team_feats_szn.columns], team1_vals))\n",
    "\n",
    "        team2_vals = team_feats_szn.loc[team_feats_szn['TeamID'] == team2].values.tolist()[0]\n",
    "        team2_dict = dict(zip([f'{col}_2' for col in team_feats_szn.columns], team2_vals))\n",
    "\n",
    "        diffs_dict = {}\n",
    "        for team_feat in team1_dict.keys():\n",
    "            \n",
    "            # remove the _# so that we can use for team2 as well\n",
    "            team_feat_stem = team_feat[:-2]\n",
    "\n",
    "            # if this is a stat-related column, find the difference between team 1 and team 2\n",
    "            if team_feat_stem in stat_feats:\n",
    "                diffs_dict[f'{team_feat_stem}_diff'] = team1_dict[f'{team_feat_stem}_1'] - team2_dict[f'{team_feat_stem}_2']\n",
    "\n",
    "            # if it's not a numeric stat column, add existing elements to dict\n",
    "            else:\n",
    "                diffs_dict[f'{team_feat_stem}_1'] = team1_dict[f'{team_feat_stem}_1']\n",
    "                diffs_dict[f'{team_feat_stem}_2'] = team2_dict[f'{team_feat_stem}_2']\n",
    "\n",
    "        all_pairwise_diffs_data.append(diffs_dict)\n",
    "\n",
    "        team1_dict.update(team2_dict)\n",
    "        all_pairwise_data.append(team1_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    pairwise_df_szn = pd.DataFrame(all_pairwise_data)\n",
    "    pairwise_diffs_df_szn = pd.DataFrame(all_pairwise_diffs_data)\n",
    "\n",
    "    # concat confs, then drop team ids and conference data\n",
    "    concat_confs = pairwise_df_szn['ConfAbbrev_1']+ '_' + pairwise_df_szn['ConfAbbrev_2']\n",
    "    pairwise_df_szn.insert(column = 'merged_conf', value = concat_confs, loc = pairwise_df_szn.shape[1])\n",
    "\n",
    "    pairwise_df_szn.drop(columns = ['ConfAbbrev_1', 'ConfAbbrev_2', 'TeamID_1', 'TeamID_2'], inplace = True)\n",
    "\n",
    "    # concat confs, then drop team ids and conference data\n",
    "    concat_confs = pairwise_diffs_df_szn['ConfAbbrev_1']+ '_' + pairwise_diffs_df_szn['ConfAbbrev_2']\n",
    "    pairwise_diffs_df_szn.insert(column = 'merged_conf', value = concat_confs, loc = pairwise_diffs_df_szn.shape[1])\n",
    "\n",
    "    pairwise_diffs_df_szn.drop(columns = ['ConfAbbrev_1', 'ConfAbbrev_2', 'TeamID_1', 'TeamID_2'], inplace = True)\n",
    "\n",
    "    return pairwise_df_szn, pairwise_diffs_df_szn\n",
    "\n",
    "\n",
    "def probs_to_df(savename, pairwise_probs, reg_szn_data = reg_szn_diffs_2025_data):\n",
    "\n",
    "    all_team_ids_2025 = [int(teamid) for teamid in reg_szn_data['TeamID'].unique()]\n",
    "    pairwise_teams_2025 = list(itertools.combinations(all_team_ids_2025, 2))\n",
    "\n",
    "    growing_list_of_probs = []\n",
    "\n",
    "    assert len(pairwise_teams_2025) == len(pairwise_probs)\n",
    "    for obs_num in range(len(pairwise_teams_2025)):\n",
    "        team1, team2 = pairwise_teams_2025[obs_num]\n",
    "        team1_winprob, team2_winprob = pairwise_probs[obs_num]\n",
    "        this_list = [team1, team1_winprob, team2, team2_winprob]\n",
    "        growing_list_of_probs.append(this_list)\n",
    "\n",
    "    pairwise_winprob_2025_df = pd.DataFrame(growing_list_of_probs)\n",
    "    pairwise_winprob_2025_df.columns = ['team1', 'winprob1', 'team2', 'winprob2']\n",
    "    pairwise_winprob_2025_df.to_csv(os.path.join(path_to_data, 'custom_datasets', savename), index=False)\n",
    "\n",
    "    return pairwise_winprob_2025_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prob_df_to_bracket_results(savename,\n",
    "                               pairwise_winprob_df,\n",
    "                               tourney_seeds = tourney_seeds,\n",
    "                               tourney_slots = tourney_slots):\n",
    "    \n",
    "\n",
    "    tourney_seeds_2025 = tourney_seeds.loc[tourney_seeds['Season'] == 2025].reset_index(drop = True)\n",
    "    tourney_slots_2025 = tourney_slots.loc[tourney_slots['Season'] == 2025].reset_index(drop = True)\n",
    "\n",
    "    # identify the play-in round games by their row number\n",
    "    play_in_rownums = list(tourney_slots_2025.loc[tourney_slots_2025['StrongSeed'].str.endswith('a')].index)\n",
    "\n",
    "    # move these rows to the top\n",
    "\n",
    "    tourney_slots_2025 = pd.concat([tourney_slots_2025.iloc[play_in_rownums], tourney_slots_2025.drop(index=play_in_rownums)], axis = 0)\n",
    "\n",
    "    tourney_seeds_2025_dict = dict(zip(tourney_seeds_2025['Seed'], tourney_seeds_2025['TeamID']))\n",
    "\n",
    "    readable_matchup_winner_list = []\n",
    "\n",
    "    for rownum, rowvals in tqdm(tourney_slots_2025.iterrows()):\n",
    "\n",
    "        teama_seed = rowvals['StrongSeed']\n",
    "        teama_id = tourney_seeds_2025_dict[teama_seed]\n",
    "        teama_school = team_data['TeamName'].loc[team_data['TeamID'] == teama_id].values[0]\n",
    "\n",
    "        teamb_seed = rowvals['WeakSeed']\n",
    "        teamb_id = tourney_seeds_2025_dict[teamb_seed]\n",
    "        teamb_school = team_data['TeamName'].loc[team_data['TeamID'] == teamb_id].values[0]\n",
    "\n",
    "        winner_slot = rowvals['Slot']\n",
    "\n",
    "        rownum = int(pairwise_winprob_df.loc[pairwise_winprob_df.apply(lambda row: frozenset([row['team1'], row['team2']]) == frozenset([teama_id, teamb_id]), axis = 1)].index.values[0])\n",
    "        this_row = pairwise_winprob_df.iloc[rownum,:]\n",
    "\n",
    "        if this_row['team1'] == teama_id:\n",
    "\n",
    "            teama_winprob = this_row['winprob1']\n",
    "            teamb_winprob = this_row['winprob2']\n",
    "\n",
    "            \n",
    "        elif this_row['team1'] == teamb_id:\n",
    "\n",
    "            teama_winprob = this_row['winprob2']\n",
    "            teamb_winprob = this_row['winprob1']\n",
    "\n",
    "        \n",
    "        if teama_winprob > teamb_winprob:\n",
    "            winner = teama_id\n",
    "        elif teamb_winprob > teama_winprob:\n",
    "            winner = teamb_id\n",
    "\n",
    "        winner_school = team_data['TeamName'].loc[team_data['TeamID'] == winner].values[0]\n",
    "\n",
    "        # update the seed dict with the winner adopting the new slot\n",
    "        tourney_seeds_2025_dict[winner_slot] = winner\n",
    "        \n",
    "        this_matchup_entry = [teama_school, teamb_school, winner_school, np.round(teama_winprob, 3), np.round(teamb_winprob, 3)]\n",
    "\n",
    "        readable_matchup_winner_list.append(this_matchup_entry)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    bracket_res_df = pd.DataFrame(readable_matchup_winner_list)\n",
    "    bracket_res_df.to_csv(os.path.join(path_to_data, 'custom_datasets', savename), index = False)\n",
    "\n",
    "    return bracket_res_df\n",
    "\n",
    "\n",
    "def train_optimal_logreg_mod(savename_stem,\n",
    "                            opt_k_dict,\n",
    "                            clf_param_dict,\n",
    "                            feature_set_to_include,\n",
    "                            opt_scaler = StandardScaler(),\n",
    "                            m_or_w = m_or_w):\n",
    "\n",
    "    # train optimal mod:\n",
    "    if m_or_w == 'MEN':\n",
    "        prefix = 'MENS'\n",
    "    elif m_or_w == 'WOMEN':\n",
    "        prefix = 'WOMENS'\n",
    "\n",
    "    train_reg_szn_diffs_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', f'{prefix}_train_diffs.csv'), index_col= 0)\n",
    "    val_reg_szn_diffs_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', f'{prefix}_val_diffs.csv'), index_col= 0)\n",
    "    reg_szn_diffs_2025_data = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', f'{prefix}_test_diffs.csv'), index_col= 0)\n",
    "    \n",
    "\n",
    "    stat_feats = list(reg_szn_diffs_2025_data.columns[2:])\n",
    "\n",
    "    train_list_of_nondiffs_dfs = []\n",
    "    train_list_of_diffs_dfs = []\n",
    "    train_list_of_ys = []\n",
    "\n",
    "    for season in train_reg_szn_diffs_data['Season'].unique():\n",
    "\n",
    "        team_elo_dict, final_elo_df = make_final_elo_df(year = season, score_result_df=raw_reg_szn, team_elo_dict=None, k_scheme=opt_k_dict, fix_k = False)\n",
    "\n",
    "\n",
    "\n",
    "        x_pairwise, x_pairwise_diffs, y = create_x_y_data(season = season, final_elo_df_szn = final_elo_df, reg_szn_data=train_reg_szn_diffs_data, \n",
    "                                                        stat_feats = stat_feats, include_winner_in_x = False, include_conf_tourney_data=True,\n",
    "                                                        include_secondary_tourney_data=False)\n",
    "\n",
    "        train_list_of_nondiffs_dfs.append(x_pairwise)\n",
    "        train_list_of_diffs_dfs.append(x_pairwise_diffs)\n",
    "        train_list_of_ys.extend(y)\n",
    "\n",
    "    full_nondiffs_df_train = pd.concat(train_list_of_nondiffs_dfs, axis = 0, ignore_index=True)\n",
    "    full_diffs_df_train = pd.concat(train_list_of_diffs_dfs, axis = 0, ignore_index=True)\n",
    "    full_y_df_train = pd.DataFrame(train_list_of_ys)\n",
    "\n",
    "    barebones_diffs_train_sepconf = full_diffs_df_train[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "    barebones_diffs_train_sepconf[['conf_1', 'conf_2']] = barebones_diffs_train_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "    barebones_diffs_train_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "    full_diffs_train_sepconf = full_diffs_df_train.copy(deep = True)\n",
    "    full_diffs_train_sepconf[['conf_1', 'conf_2']] = full_diffs_train_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "    full_diffs_train_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "\n",
    "    ######################################### validation data\n",
    "    val_list_of_nondiffs_dfs = []\n",
    "    val_list_of_diffs_dfs = []\n",
    "    val_list_of_ys = []\n",
    "\n",
    "    for season in val_reg_szn_diffs_data['Season'].unique():\n",
    "\n",
    "        team_elo_dict, final_elo_df = make_final_elo_df(year = season, score_result_df=raw_reg_szn, team_elo_dict=None, k_scheme=opt_k_dict, fix_k = False)\n",
    "\n",
    "        x_pairwise, x_pairwise_diffs, y = create_x_y_data(season = season, final_elo_df_szn = final_elo_df, reg_szn_data=val_reg_szn_diffs_data, stat_feats=stat_feats,\n",
    "                                                        include_winner_in_x = False, include_conf_tourney_data=True,\n",
    "                                                        include_secondary_tourney_data=False)\n",
    "\n",
    "        val_list_of_nondiffs_dfs.append(x_pairwise)\n",
    "        val_list_of_diffs_dfs.append(x_pairwise_diffs)\n",
    "        val_list_of_ys.extend(y)\n",
    "\n",
    "    full_nondiffs_df_val = pd.concat(val_list_of_nondiffs_dfs, axis = 0, ignore_index=True)\n",
    "    full_diffs_df_val = pd.concat(val_list_of_diffs_dfs, axis = 0, ignore_index=True)\n",
    "    full_y_df_val = pd.DataFrame(val_list_of_ys)\n",
    "\n",
    "    barebones_diffs_val_sepconf = full_diffs_df_val[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "    barebones_diffs_val_sepconf[['conf_1', 'conf_2']] = barebones_diffs_val_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "    barebones_diffs_val_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "    full_diffs_val_sepconf = full_diffs_df_val.copy(deep = True)\n",
    "    full_diffs_val_sepconf[['conf_1', 'conf_2']] = full_diffs_val_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "    full_diffs_val_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "\n",
    "    ######################################### 2025 data\n",
    "    full_nondiffs_df_test, full_diffs_df_test = make_2025_x()\n",
    "\n",
    "    barebones_diffs_test_sepconf = full_diffs_df_test[['elo_1', 'elo_2', 'merged_conf']].copy(deep = True)\n",
    "    barebones_diffs_test_sepconf[['conf_1', 'conf_2']] = barebones_diffs_test_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "    barebones_diffs_test_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "    full_diffs_test_sepconf = full_diffs_df_test.copy(deep = True)\n",
    "    full_diffs_test_sepconf[['conf_1', 'conf_2']] = full_diffs_test_sepconf['merged_conf'].str.split('_', n = 1, expand = True)\n",
    "    full_diffs_test_sepconf.drop(columns = 'merged_conf', inplace = True)\n",
    "\n",
    "\n",
    "    ######################################### modeling\n",
    "    \n",
    "    clf = LogisticRegression(**clf_param_dict)\n",
    "\n",
    "    if 'sepconf' in feature_set_to_include:\n",
    "        params_to_categorize = ['conf_1', 'conf_2']\n",
    "    else:\n",
    "        params_to_categorize = ['merged_conf']\n",
    "\n",
    "    \n",
    "\n",
    "    if feature_set_to_include == 'full_sepconf_diffs':\n",
    "\n",
    "        for param in params_to_categorize:\n",
    "            full_diffs_test_sepconf[param] = full_diffs_test_sepconf[param].astype('category')\n",
    "\n",
    "        scale_cols = [col for col in full_diffs_test_sepconf.columns if col not in params_to_categorize]\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', opt_scaler, scale_cols), \n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), params_to_categorize)\n",
    "        ], remainder='passthrough')  \n",
    "            \n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor), \n",
    "            ('logreg', clf)  \n",
    "        ])\n",
    "\n",
    "\n",
    "        pipeline.fit(full_diffs_train_sepconf, full_y_df_train.values.ravel())\n",
    "        pairwise_probs_2025 = pipeline.predict_proba(full_diffs_test_sepconf)\n",
    "\n",
    "    elif feature_set_to_include == 'barebones_diffs_sepconf':\n",
    "\n",
    "        for param in params_to_categorize:\n",
    "            barebones_diffs_test_sepconf[param] = barebones_diffs_test_sepconf[param].astype('category')\n",
    "\n",
    "        scale_cols = [col for col in barebones_diffs_test_sepconf.columns if col not in params_to_categorize]\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', opt_scaler, scale_cols), \n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), params_to_categorize)\n",
    "        ], remainder='passthrough')  \n",
    "            \n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor), \n",
    "            ('logreg', clf)  \n",
    "        ])\n",
    "\n",
    "        pipeline.fit(barebones_diffs_train_sepconf, full_y_df_train.values.ravel())\n",
    "        pairwise_probs_2025 = pipeline.predict_proba(barebones_diffs_test_sepconf)\n",
    "\n",
    "    elif feature_set_to_include == 'full_diffs':\n",
    "\n",
    "        for param in params_to_categorize:\n",
    "            full_diffs_df_test[param] = full_diffs_df_test[param].astype('category')\n",
    "\n",
    "        scale_cols = [col for col in full_diffs_df_test.columns if col not in params_to_categorize]\n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('num', opt_scaler, scale_cols), \n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), params_to_categorize)\n",
    "        ], remainder='passthrough')  \n",
    "            \n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor), \n",
    "            ('logreg', clf)  \n",
    "        ])\n",
    "        \n",
    "        pipeline.fit(full_diffs_df_train, full_y_df_train.values.ravel())\n",
    "        pairwise_probs_2025 = pipeline.predict_proba(full_diffs_df_test)\n",
    "\n",
    "    pairwise_winprob_2025_df = probs_to_df(savename = f'{savename_stem}_{m_or_w}_pairwise_probs_2025.csv', pairwise_probs = pairwise_probs_2025, reg_szn_data = reg_szn_diffs_2025_data)\n",
    "\n",
    "    bracket_res_df = prob_df_to_bracket_results(savename = f'{savename_stem}_{m_or_w}_bracket_res_df.csv',\n",
    "                                                pairwise_winprob_df = pairwise_winprob_2025_df,\n",
    "                                                tourney_seeds = tourney_seeds,\n",
    "                                                tourney_slots = tourney_slots)\n",
    "    \n",
    "    return pairwise_winprob_2025_df, bracket_res_df\n",
    "\n",
    "\n",
    "def grid_search(conf_game_k_vals,\n",
    "                ooc_game_k_vals,\n",
    "                fixed_game_k_vals,\n",
    "                run_name_stem,\n",
    "                this_include_conf_res = True,\n",
    "                this_include_secondary_res = False,\n",
    "                these_scalers_to_include = [StandardScaler()],\n",
    "                these_feature_sets_to_include = ['full_sepconf_diffs'],\n",
    "                these_mods_to_include = ['logreg'],\n",
    "                this_reg_szn_data_type = 'DIFFS'):\n",
    "    \n",
    "    '''\n",
    "    if include_conf_res, we will NOT adjust ELO from conference tournament results\n",
    "    '''\n",
    "\n",
    "    # grid searching for optimal params\n",
    "\n",
    "    dict_combos = list(itertools.product(conf_game_k_vals, ooc_game_k_vals, fixed_game_k_vals))\n",
    "    keys = ['conf', 'ooc', 'fixed']\n",
    "    list_of_k_schemes = [dict(zip(keys, dict_combo)) for dict_combo in dict_combos] \n",
    "\n",
    "    for dict_num, k_dict in enumerate(list_of_k_schemes):\n",
    "        k_dict['name'] = f'k_{run_name_stem}_{dict_num}'\n",
    "\n",
    "    k_scheme_df = pd.DataFrame(list_of_k_schemes)\n",
    "\n",
    "    k_scheme_dir_path = os.path.join(path_to_data, 'custom_datasets', 'k_schemes')\n",
    "\n",
    "    if not os.path.exists(k_scheme_dir_path):\n",
    "        os.makedirs(k_scheme_dir_path)\n",
    "    k_scheme_df.to_csv(os.path.join(k_scheme_dir_path, f'gridsearch_{run_name_stem}.csv'))\n",
    "\n",
    "    for dict_num, this_k_dict in enumerate(list_of_k_schemes):\n",
    "    \n",
    "        # full_workflow(this_k_dict, data_type)\n",
    "        full_workflow(k_dict = this_k_dict, \n",
    "                    reg_szn_data_type = this_reg_szn_data_type, \n",
    "                    include_conf_res = this_include_conf_res, \n",
    "                    include_secondary_res = this_include_secondary_res,\n",
    "                    feature_sets_to_include = these_feature_sets_to_include,\n",
    "                    mods_to_include = these_mods_to_include,\n",
    "                    scalers_to_include = these_scalers_to_include,\n",
    "                    run_name = run_name_stem)\n",
    "    \n",
    "    print(f'finished dict {dict_num} / {len(list_of_k_schemes)}')\n",
    "\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## one submission will be full_sepconf for both men's and women's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### FULL_SEPCONF (SUBMISSION 1), MEN'S\n",
    "\n",
    "# # optimal params from men's grid search ...\n",
    "\n",
    "# this_opt_k_dict = {'ooc':15,\n",
    "#               'conf': 40}\n",
    "# these_opt_clf_params = {'solver': 'liblinear',\n",
    "#                         'penalty': 'l2',\n",
    "#                         'max_iter': 100, \n",
    "#                         'C': 1,\n",
    "#                         'random_state': 42}\n",
    "\n",
    "# mens_opt_full_pairwise_winprob_2025_df, mens_opt_full_bracket_res_df = train_optimal_logreg_mod(savename_stem = 'Mopt_full_sepconf_standard',\n",
    "#                                                                                                     opt_k_dict = this_opt_k_dict,\n",
    "#                                                                                                     clf_param_dict = these_opt_clf_params,\n",
    "#                                                                                                     feature_set_to_include = 'full_sepconf_diffs',\n",
    "#                                                                                                     opt_scaler=StandardScaler(),\n",
    "#                                                                                                     m_or_w = m_or_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5140it [00:02, 2265.17it/s]\n",
      "5138it [00:02, 2211.51it/s]\n",
      "5184it [00:02, 2276.32it/s]\n",
      "5252it [00:02, 2192.31it/s]\n",
      "5214it [00:02, 2328.74it/s]\n",
      "5210it [00:02, 2175.77it/s]\n",
      "5209it [00:02, 2237.43it/s]\n",
      "3556it [00:01, 2214.15it/s]\n",
      "5060it [00:02, 2284.63it/s]\n",
      "5414it [00:02, 2290.95it/s]\n",
      "5114it [00:02, 2291.53it/s]\n",
      "5209it [00:02, 2293.19it/s]\n",
      "5240it [00:02, 2263.39it/s]\n",
      "5374it [00:02, 2319.11it/s]\n",
      "5444it [00:02, 2301.39it/s]\n",
      "100%|| 65341/65341 [00:24<00:00, 2715.95it/s]\n",
      "67it [00:29,  2.24it/s]\n"
     ]
    }
   ],
   "source": [
    "#### FULL_SEPCONF (SUBMISSION 1), WOMEN'S\n",
    "\n",
    "# optimal params from women's grid search ...\n",
    "\n",
    "this_opt_k_dict = {'ooc':20,\n",
    "                   'conf': 10}\n",
    "these_opt_clf_params = {'solver': 'liblinear',\n",
    "                        'penalty': 'l2',\n",
    "                        'max_iter': 100, \n",
    "                        'C': 10,\n",
    "                        'random_state': 42}\n",
    "\n",
    "womens_opt_full_pairwise_winprob_2025_df, womens_opt_full_bracket_res_df = train_optimal_logreg_mod(savename_stem = 'Wopt_full_sepconf_diffs_minmax',\n",
    "                                                                                            opt_k_dict = this_opt_k_dict,\n",
    "                                                                                            clf_param_dict = these_opt_clf_params,\n",
    "                                                                                            feature_set_to_include = 'full_sepconf_diffs',\n",
    "                                                                                            opt_scaler=MinMaxScaler(),\n",
    "                                                                                            m_or_w = m_or_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## the other submission will be barebones_sepconf for both men's and women's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### BAREBONES_SEPCONF (SUBMISSION 2), MEN'S\n",
    "\n",
    "# # optimal params from men's grid search ...\n",
    "\n",
    "# this_opt_k_dict = {'ooc':20,\n",
    "#                    'conf': 40}\n",
    "# these_opt_clf_params = {'solver': 'liblinear',\n",
    "#                         'penalty': 'l2',\n",
    "#                         'max_iter': 100, \n",
    "#                         'C': 1,\n",
    "#                         'random_state': 42}\n",
    "\n",
    "# mens_opt_bb_pairwise_winprob_2025_df, mens_opt_bb_bracket_res_df = train_optimal_logreg_mod(savename_stem = 'Mopt_barebones_sepconf_diffs_standard',\n",
    "#                                                                                             opt_k_dict = this_opt_k_dict,\n",
    "#                                                                                             clf_param_dict = these_opt_clf_params,\n",
    "#                                                                                             feature_set_to_include = 'barebones_diffs_sepconf',\n",
    "#                                                                                             opt_scaler=StandardScaler(),\n",
    "#                                                                                             m_or_w = m_or_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5140it [00:02, 2299.02it/s]\n",
      "5138it [00:02, 2316.85it/s]\n",
      "5184it [00:02, 2288.79it/s]\n",
      "5252it [00:02, 2306.27it/s]\n",
      "5214it [00:02, 2301.00it/s]\n",
      "5210it [00:02, 2307.46it/s]\n",
      "5209it [00:02, 2312.33it/s]\n",
      "3556it [00:01, 2274.15it/s]\n",
      "5060it [00:02, 2106.32it/s]\n",
      "5414it [00:02, 2270.21it/s]\n",
      "5114it [00:02, 2265.58it/s]\n",
      "5209it [00:02, 2212.94it/s]\n",
      "5240it [00:02, 2274.70it/s]\n",
      "5374it [00:02, 2288.63it/s]\n",
      "5444it [00:02, 2241.47it/s]\n",
      "100%|| 65341/65341 [00:25<00:00, 2590.61it/s]\n",
      "67it [00:32,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "#### BAREBONES_SEPCONF (SUBMISSION 2), WOMEN'S\n",
    "\n",
    "# optimal params from women's grid search ...\n",
    "\n",
    "this_opt_k_dict = {'ooc':15,\n",
    "                   'conf': 10}\n",
    "these_opt_clf_params = {'solver': 'liblinear',\n",
    "                        'penalty': 'l2',\n",
    "                        'max_iter': 100, \n",
    "                        'C': 1,\n",
    "                        'random_state': 42}\n",
    "\n",
    "womens_opt_bb_pairwise_winprob_2025_df, womens_opt_bb_bracket_res_df = train_optimal_logreg_mod(savename_stem = 'Wopt_barebones_sepconf_diffs_minmax',\n",
    "                                                                                            opt_k_dict = this_opt_k_dict,\n",
    "                                                                                            clf_param_dict = these_opt_clf_params,\n",
    "                                                                                            feature_set_to_include = 'barebones_diffs_sepconf',\n",
    "                                                                                            opt_scaler=MinMaxScaler(),\n",
    "                                                                                            m_or_w = m_or_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission_format(pairwise_winprob_df):\n",
    "\n",
    "    # growing_winprob_dict will be a dict of dicts\n",
    "    growing_winprob_dict = {}\n",
    "    sorted_team1 = pairwise_winprob_df.sort_values(by=['team1', 'team2'], ascending=[True, True])\n",
    "    for rownum, rowvals in tqdm(sorted_team1.iterrows()):\n",
    "\n",
    "        team1 = int(rowvals['team1'])\n",
    "        team2 = int(rowvals['team2'])\n",
    "\n",
    "        smaller_team = min([team1, team2])\n",
    "        larger_team = max([team1, team2])\n",
    "\n",
    "        # \"You must predict the probability that the team with the lower TeamId beats the team with the higher TeamId\"\n",
    "        if smaller_team == team1:\n",
    "            win_prob = rowvals['winprob1']\n",
    "        elif smaller_team == team2:\n",
    "            win_prob = rowvals['winprob2']\n",
    "        \n",
    "        if smaller_team in growing_winprob_dict.keys():\n",
    "            growing_winprob_dict[smaller_team][larger_team] = win_prob\n",
    "        elif smaller_team not in growing_winprob_dict.keys():\n",
    "            growing_winprob_dict[smaller_team] = {larger_team:win_prob}\n",
    "\n",
    "    # convert dict of dicts into a dataframe\n",
    "    rows = [(smaller_team_id, larger_team_id, smaller_team_winprob) \n",
    "            for smaller_team_id, inner_dict in growing_winprob_dict.items() \n",
    "            for larger_team_id, smaller_team_winprob in inner_dict.items()]\n",
    "\n",
    "    unsorted_df = pd.DataFrame(rows, columns = ['smaller_id', 'larger_id', 'Pred'])\n",
    "    sorted_df = unsorted_df.sort_values(['smaller_id', 'larger_id'])\n",
    "\n",
    "    id_col_vals = sorted_df.apply(lambda row: f'2025_{int(row['smaller_id'])}_{int(row['larger_id'])}', axis = 1)\n",
    "\n",
    "    sorted_df.drop(columns = ['smaller_id', 'larger_id'], inplace=True)\n",
    "    sorted_df.insert(loc = 0, column = 'ID', value = id_col_vals)\n",
    "\n",
    "    return sorted_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66066it [00:01, 39377.53it/s]\n",
      "65341it [00:01, 40042.09it/s]\n",
      "66066it [00:01, 41531.35it/s]\n",
      "65341it [00:01, 40004.73it/s]\n"
     ]
    }
   ],
   "source": [
    "# prepare submission file:\n",
    "# assumes both the men's and women's data have been generated already:\n",
    "\n",
    "FULL_mens_opt_pairwise_winprob_2025_df = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'Mopt_full_sepconf_standard_MEN_pairwise_probs_2025.csv'), index_col=False)\n",
    "FULL_womens_opt_pairwise_winprob_2025_df = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'Wopt_full_sepconf_diffs_minmax_WOMEN_pairwise_probs_2025.csv'), index_col=False)\n",
    "\n",
    "BAREBONES_mens_opt_pairwise_winprob_2025_df = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'Mopt_barebones_sepconf_diffs_standard_MEN_pairwise_probs_2025.csv'), index_col=False)\n",
    "BAREBONES_womens_opt_pairwise_winprob_2025_df = pd.read_csv(os.path.join(path_to_data, 'custom_datasets', 'Wopt_barebones_sepconf_diffs_minmax_WOMEN_pairwise_probs_2025.csv'), index_col=False)\n",
    "\n",
    "\n",
    "mens_full_optim = make_submission_format(FULL_mens_opt_pairwise_winprob_2025_df)\n",
    "womens_full_optim = make_submission_format(FULL_womens_opt_pairwise_winprob_2025_df)\n",
    "\n",
    "mens_barebones_optim = make_submission_format(BAREBONES_mens_opt_pairwise_winprob_2025_df)\n",
    "womens_barebones_optim = make_submission_format(BAREBONES_womens_opt_pairwise_winprob_2025_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission csvs\n",
    "\n",
    "full_submission1_df = pd.concat([mens_full_optim, womens_full_optim], axis = 0).reset_index(drop = True)\n",
    "full_submission1_df.to_csv(os.path.join(path_to_data, 'custom_datasets', 'full_submission1.csv'), index = False, header=True)\n",
    "\n",
    "bb_submission2_df = pd.concat([mens_barebones_optim, womens_barebones_optim], axis = 0).reset_index(drop = True)\n",
    "bb_submission2_df.to_csv(os.path.join(path_to_data, 'custom_datasets', 'bb_submission2.csv'), index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "(131407, 2)\n",
      "(131407, 2)\n",
      "(131407, 2)\n"
     ]
    }
   ],
   "source": [
    "# verfiy that the IDs are the same between the example stage2 submission csv and the csvs generated above\n",
    "sample_submission = pd.read_csv(os.path.join(path_to_data, 'SampleSubmissionStage2.csv'))\n",
    "\n",
    "full_num_differences = (full_submission1_df['ID'] != sample_submission['ID']).sum()\n",
    "print(full_num_differences)\n",
    "\n",
    "bb_num_differences = (bb_submission2_df['ID'] != sample_submission['ID']).sum()\n",
    "print(bb_num_differences)\n",
    "\n",
    "print(full_submission1_df.shape)\n",
    "print(bb_submission2_df.shape)\n",
    "print(sample_submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fun functions that aren't used in the final analyses but helped with EDA\n",
    "\n",
    "def find_slot_path(seed, tourney_slots):\n",
    "    # championship slot ends in CH\n",
    "    championship_slot = tourney_slots['Slot'][tourney_slots['Slot'].str.endswith('CH')].values[0]\n",
    "    next_slot = tourney_slots['Slot'].loc[(tourney_slots['StrongSeed'] == seed) | (tourney_slots['WeakSeed'] == seed)].values[0]\n",
    "    poss_slots = [next_slot]\n",
    "    while next_slot != championship_slot:\n",
    "        next_slot = tourney_slots['Slot'].loc[(tourney_slots['StrongSeed'] == next_slot) | (tourney_slots['WeakSeed'] == next_slot)].values[0]\n",
    "        poss_slots.append(next_slot)\n",
    "    return poss_slots\n",
    "\n",
    "def games_before_meeting(seed1, seed2, tourney_slots):\n",
    "    slot_path1 = find_slot_path(seed1, tourney_slots)\n",
    "    slot_path2 = find_slot_path(seed2, tourney_slots)\n",
    "\n",
    "    for ind1, slot1 in enumerate(slot_path1):\n",
    "        for ind2, slot2 in enumerate(slot_path2):\n",
    "            if slot1 == slot2:\n",
    "                return ind1, ind2\n",
    "            \n",
    "def create_tournament_matchup_dict(season):\n",
    "    '''\n",
    "    Was used for intermediate analyses; not included in final workflow\n",
    "    '''\n",
    "    season_seeds = tourney_seeds.loc[tourney_seeds['Season'] == season]\n",
    "    seed_team_dict = dict(zip(season_seeds['Seed'], season_seeds['TeamID']))\n",
    "\n",
    "    season_tourney_slots = tourney_slots.loc[tourney_slots['Season'] == season]\n",
    "    season_tourney_results = raw_mm_tourn.loc[raw_mm_tourn['Season'] == season]\n",
    "\n",
    "    round_matchup_dict = {}\n",
    "    for rownum, rowvals in season_tourney_slots.iterrows():\n",
    "        game_slot = rowvals['Slot']\n",
    "        strong_seed, weak_seed = season_tourney_slots[['StrongSeed', 'WeakSeed']].loc[season_tourney_slots['Slot'] == game_slot].values.tolist()[0]\n",
    "\n",
    "        try:\n",
    "            team1 = seed_team_dict[strong_seed]\n",
    "            team2 = seed_team_dict[weak_seed]\n",
    "\n",
    "        except KeyError as e: \n",
    "\n",
    "            # account for the fact that there might be play-in games ... \n",
    "            # assign the non-a/b listed seed as the winner of the a vs b game\n",
    "\n",
    "            if strong_seed not in seed_team_dict.keys():\n",
    "                problematic_seed = strong_seed\n",
    "            elif weak_seed not in seed_team_dict.keys():\n",
    "                problematic_seed = weak_seed\n",
    "        \n",
    "            # strip away the a and b, get both combos, search for winner of that game ... \n",
    "            play_in_team_a = seed_team_dict[f'{problematic_seed}a']\n",
    "            play_in_team_b = seed_team_dict[f'{problematic_seed}b']\n",
    "\n",
    "            play_in_winner = int(season_tourney_results.loc[season_tourney_results.apply(lambda row: frozenset([row['WTeamID'], row['LTeamID']]) == frozenset([play_in_team_a, play_in_team_b]), axis = 1), 'WTeamID'].values[0])\n",
    "\n",
    "            # add this play_in_winner as the official seed in the dict\n",
    "            seed_team_dict[problematic_seed] = play_in_winner\n",
    "\n",
    "            team1 = seed_team_dict[strong_seed]\n",
    "            team2 = seed_team_dict[weak_seed]\n",
    "\n",
    "        winner = int(season_tourney_results.loc[season_tourney_results.apply(lambda row: frozenset([row['WTeamID'], row['LTeamID']]) == frozenset([team1, team2]), axis = 1), 'WTeamID'].values[0])\n",
    "\n",
    "        # add the winner as a new entry with the slot name as key\n",
    "        seed_team_dict[game_slot] = winner\n",
    "\n",
    "        round_num = int(seed_slots['GameRound'].loc[seed_slots['GameSlot'] == game_slot].values[0])\n",
    "        if round_num in round_matchup_dict.keys():\n",
    "            round_matchup_dict[round_num].append([team1, team2, winner])\n",
    "        else:\n",
    "            round_matchup_dict[round_num] = [[team1, team2, winner]]\n",
    "\n",
    "    return round_matchup_dict\n",
    "\n",
    "\n",
    "\n",
    "def find_brier_score(tourney_results, pairwise_matchup_probs):\n",
    "    '''\n",
    "    get the brier score for a given set of matchup probs and tournament results\n",
    "    '''\n",
    "    running_error = 0\n",
    "    running_matchup_counter = 0\n",
    "\n",
    "    for round_num,matchup_list in tourney_results.items():\n",
    "        for matchup in matchup_list:\n",
    "            winner = matchup[2]\n",
    "            teams = set(matchup)\n",
    "            loser = list(teams.difference(set([winner])))[0]\n",
    "            outcome_prob = pairwise_matchup_probs[winner][loser]\n",
    "\n",
    "            squared_loss = (1-outcome_prob)**2\n",
    "            running_error += squared_loss\n",
    "            running_matchup_counter += 1\n",
    "\n",
    "    return running_error / running_matchup_counter\n",
    "\n",
    "\n",
    "def prelim_analysis(year, k_dict, home_court_advantage = 0):\n",
    "\n",
    "\n",
    "    team_elo_dict, final_elo_df = make_final_elo_df(year = year, score_result_df=raw_reg_szn, home_team_adjustment = home_court_advantage, \n",
    "                                                    team_elo_dict=None, k_scheme=k_dict, fix_k = False)\n",
    "\n",
    "    postcon_team_elo_dict, postcon_final_elo_df = make_final_elo_df(year = year, score_result_df=raw_conf_tourn, home_team_adjustment = 0, \n",
    "                                                                    team_elo_dict=team_elo_dict, k_scheme=k_dict, fix_k= True)\n",
    "\n",
    "    final_postcon_team_elo_dict = {team:elo[-1] for team, elo in postcon_team_elo_dict.items()}\n",
    "\n",
    "    round_matchup_dict_2024 = create_tournament_matchup_dict(2024)\n",
    "\n",
    "    pairwise_winprob_dict = {}\n",
    "\n",
    "    for team1, elo1 in final_postcon_team_elo_dict.items():\n",
    "        pairwise_winprob_dict[team1] = {}\n",
    "        for team2, elo2 in final_postcon_team_elo_dict.items():\n",
    "            # no home court boost in march madness\n",
    "            pairwise_winprob_dict[team1][team2] = get_elo_win_prob(elo1, elo2, home_court_advantage_boost_1 = 0, home_court_advantage_boost_2 = 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    brier_score = find_brier_score(tourney_results = round_matchup_dict_2024, \n",
    "                                pairwise_matchup_probs = pairwise_winprob_dict)\n",
    "\n",
    "    return(brier_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example women's grid search ... \n",
    "\n",
    "# grid_search(conf_game_k_vals = [20, 30, 40, 50],\n",
    "#             ooc_game_k_vals = [10, 20, 30, 40],\n",
    "#             fixed_game_k_vals = [1],\n",
    "#             this_include_conf_res = True,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs', 'barebones_sepconf_diffs', 'full_diffs'],\n",
    "#             these_mods_to_include=['logreg', 'xgb'],\n",
    "#             run_name_stem = 'womens_long_gridsearch',\n",
    "#             )\n",
    "\n",
    "# # DO update elo in conference tourney games\n",
    "# # DONT include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [10, 20, 30, 40],\n",
    "#             ooc_game_k_vals = [10, 20],\n",
    "#             fixed_game_k_vals = [10, 20, 30],\n",
    "#             this_include_conf_res = False,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs', 'barebones_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_long_gridsearch_long_UPDATECONF_NOSEC',\n",
    "#             )\n",
    "\n",
    "# # low K values all around\n",
    "# # DONT update elo in conference tourney games\n",
    "# # DONT include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [5, 10, 15, 20, 25],\n",
    "#             ooc_game_k_vals = [5, 10, 15, 20],\n",
    "#             fixed_game_k_vals = [1],\n",
    "#             this_include_conf_res = True,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs', 'barebones_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_lowKs_DONTUPDATECONF_NOSEC',\n",
    "#             )\n",
    "\n",
    "\n",
    "# # low K values all around\n",
    "# # minmax scale ... \n",
    "# # DONT update elo in conference tourney games\n",
    "# # DONT include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [5, 10, 15, 20, 25],\n",
    "#             ooc_game_k_vals = [5, 10, 15, 20],\n",
    "#             fixed_game_k_vals = [1],\n",
    "#             this_include_conf_res = True,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [MinMaxScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs', 'barebones_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_lowKs_DONTUPDATECONF_NOSEC_minmax',\n",
    "#             )\n",
    "\n",
    "# # HIGH CONF\n",
    "# # DO update elo in conference tourney games\n",
    "# # DONT include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [60, 70, 80, 90],\n",
    "#             ooc_game_k_vals = [10, 20, 60],\n",
    "#             fixed_game_k_vals = [30, 40, 50],\n",
    "#             this_include_conf_res = False,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_long_gridsearch_long_UPDATECONF_NOSEC_HIGHCONF',\n",
    "#             )\n",
    "\n",
    "# # HIGH CONF\n",
    "# # DONT update elo in conference tourney games\n",
    "# # DONT include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [60, 70, 80, 90, 100],\n",
    "#             ooc_game_k_vals = [10, 20],\n",
    "#             fixed_game_k_vals = [1],\n",
    "#             this_include_conf_res = True,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_long_gridsearch_long_DONTUPDATECONF_NOSEC_HIGHCONF',\n",
    "#             )\n",
    "\n",
    "# # HIGH CONF but with barebones\n",
    "# # DO update elo in conference tourney games\n",
    "# # DONT include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [60, 70, 80, 90],\n",
    "#             ooc_game_k_vals = [10, 20, 60],\n",
    "#             fixed_game_k_vals = [30, 40, 50],\n",
    "#             this_include_conf_res = False,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['barebones_sepconf_nondiffs', 'barebones_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_long_gridsearch_long_UPDATECONF_NOSEC_HIGHCONF_BAREBONES',\n",
    "#             )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # HIGH CONF but with barebones\n",
    "# # DONT update elo in conference tourney games\n",
    "# # DONT include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [60, 70, 80, 90],\n",
    "#             ooc_game_k_vals = [10, 20, 60],\n",
    "#             fixed_game_k_vals = [1],\n",
    "#             this_include_conf_res = True,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['barebones_sepconf_nondiffs', 'barebones_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_long_gridsearch_long_DONTUPDATECONF_NOSEC_HIGHCONF_BAREBONES',\n",
    "#             )\n",
    "\n",
    "\n",
    "# # HIGH CONF but with barebones\n",
    "# # DONT update elo in conference tourney games\n",
    "# # DO include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [60, 70, 80, 90],\n",
    "#             ooc_game_k_vals = [10, 20, 60],\n",
    "#             fixed_game_k_vals = [1],\n",
    "#             this_include_conf_res = True,\n",
    "#             this_include_secondary_res = True,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['barebones_sepconf_nondiffs', 'barebones_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_long_gridsearch_long_DONTUPDATECONF_YESSEC_HIGHCONF_BAREBONES',\n",
    "#             )\n",
    "\n",
    "# # DO update elo in conference tourney games\n",
    "# # DO include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [10, 20, 30, 40],\n",
    "#             ooc_game_k_vals = [10, 20],\n",
    "#             fixed_game_k_vals = [10, 20, 30],\n",
    "#             this_include_conf_res = False,\n",
    "#             this_include_secondary_res = True,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_long_gridsearch_long_UPDATECONF_INCSEC',\n",
    "#             )\n",
    "\n",
    "# # DONT update elo in conference tourney games\n",
    "# # DO include secondary tourney data to train\n",
    "\n",
    "# grid_search(conf_game_k_vals = [10, 20, 30, 40],\n",
    "#             ooc_game_k_vals = [10, 20],\n",
    "#             fixed_game_k_vals = [10, 20, 30],\n",
    "#             this_include_conf_res = True,\n",
    "#             this_include_secondary_res = True,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'womens_long_gridsearch_long_DONTUPDATECONF_INCSEC',\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # men's (among others)\n",
    "# # validate men's params\n",
    "# grid_search(conf_game_k_vals = [40],\n",
    "#             ooc_game_k_vals = [15],\n",
    "#             fixed_game_k_vals = [1],\n",
    "#             this_include_conf_res = True,\n",
    "#             this_include_secondary_res = False,\n",
    "#             these_scalers_to_include = [StandardScaler()],\n",
    "#             these_feature_sets_to_include = ['full_sepconf_diffs'],\n",
    "#             these_mods_to_include=['logreg'],\n",
    "#             run_name_stem = 'mens_validate_final_params',\n",
    "#             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mm_pytorch_kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
